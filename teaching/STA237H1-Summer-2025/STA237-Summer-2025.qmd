---
title: "STA237: Probability, Statistics and Data Analysis I"
image: dice.png
categories: [Probability, R, Simulations]
description: 'Notes for STA237 Summer 2025'
toc-depth: 1
---

Acknowledgements: This page contains tutorial notes for [STA237H1 Probability, Statistics and Data Analysis I](https://artsci.calendar.utoronto.ca/course/sta237h1) Summer 2025 under the instruction of [Dr. Gracia Yunruo Dong](https://graciadong.github.io/).

# Week 1

::: {.callout-question collapse="true"}
### Question 1

b)  Suppose $\Omega = \{1, 2, 3, 4, 5, 6\}$ and let $A = \{1, 2, 4, 5\}$, $B = \{1, 3, 5\}$ and $C = \{2, 4, 6\}$. Use this example to verify the two results in the distributive law.
:::

::: {.callout-tip collapse="true"}
### Solution

We need to verify two things. \newline i. $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$

\begin{align*}
B \cup C &=  \{1, 2, 3, 4, 5, 6\} \\
A \cap (B \cup C) &=  \{1, 2, 4, 5\} \\
A \cap B &=  \{1, 5\} \\
A \cap C &=  \{2, 4\} \\
(A \cap B) \cup (A \cap C) &=  \{1, 2, 4, 5\}
\end{align*}

Therefore, $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$ as required.

\bigskip

ii. $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$

\begin{align*}
B \cap C &= \{1, 3, 5\} \cap \{2, 4, 6\} = \emptyset \\
A \cup (B \cap C) &=  \{1, 2, 4, 5\} \\
A \cup B &=  \{1, 2, 3, 4, 5\} \\
A \cup C &=  \{1, 2, 4, 5, 6\} \\
(A \cup B) \cap (A \cup C) &= \{1, 2, 4, 5\}
\end{align*}

Therefore, $A \cup (B \cap C) = (A \cup B) \cap (A \cup C)$ as required.
:::

::: {.callout-question collapsed="true"}
### Question 2

In R, write R code to simulate rolling a fair six-sided die once, 10 times, 100 times, and 1000 times.

a)  Estimate the probability of rolling a 3 or higher based on 1000 simulated dice rolls.
b)  If you were to repeat your simulation, would you end up with the same estimate? Why or why not.
:::

::: {.callout-tip collapse="true"}
### Solution

We first simulate the rolling of a fair six-sided die 10, 100, and 1000 times.

```{r}
set.seed(2025) # set seed for reproducibility
sample(1:6, 10, replace=TRUE)  # Rolling a fair six-sided die 10 times
sample(1:6, 100, replace=TRUE) # 100 times
sample(1:6, 1000, replace=TRUE) # 1000 times
```

The probability of rolling a 3 or higher based on 1000 simulated dice rolls is

```{r}
set.seed(2025) # set seed for reproducibility
mean(sample(1:6,1000,replace=TRUE)) 
```

If we were to repeat this simulation (without setting seed) five times, we get

```{r}
mean(sample(1:6,1000,replace=TRUE)) 
mean(sample(1:6,1000,replace=TRUE)) 
mean(sample(1:6,1000,replace=TRUE)) 
mean(sample(1:6,1000,replace=TRUE)) 
mean(sample(1:6,1000,replace=TRUE)) 
```

So no, every time this is repeated, we will get different sequences of simulated rolls, and will therefore have different estimates for the probability of rolling 3+.

Hence, it is important to set a seed for reproducibility purposes so other people can confirm our results.
:::

::: {.callout-question collapsed="true"}
### Question 3

Consider the word ‘STATISTICS’. Is the number of unique arrangements of the letters in ‘STATISTICS’ 10!? Justify your answer and compute the probability that a random rearrangement of the letters in ‘STATISTICS’ will spell the word ‘STATISTICS’.
:::

:::: {.callout-tip collapse="true"}
### Solution

No, because some letters are repeated. The word "STATISTICS" has 10 letters in total, but with repeated letters: S 3 times, T 3 times, I 2 times, and A, C appear once each.

To count the number of distinct permutations, we divide 10! by the factorials of the repeated letters: $\dfrac{10!}{3! \times 3! \times 2!} = \dfrac{3628800}{72} = 50400$.

::: callout-note
**Theorem: Permutations of a multiset**.

*Let S be a set with n (not necessarily distinct objects), such that there are* $n_1$ objects of type 1, $n_2$ objects of type 2, $\cdots$, $n_k$ objects of type k, where $n_1 + n_2 + ... + n_k = n$. Then the number of arrangements of these objects is $\dfrac{n!}{n_1! \times n_2! \times \cdots \times n_k!}$
:::

Since there is only one unique arrangement of the word "STATISTICS", the probability is $\boxed{\dfrac{1}{50400}}$

```{r}
numerator <- 1
denominator <- factorial(10) / (factorial(3)*factorial(3)*factorial(2))

(prob <- numerator/denominator)
```
::::

::: {.callout-question collapsed="true"}
### Question 4

Assume birthdays are equally likely to occur in each of the 12 months of the year. What is the probability that at least two people in a group of three students have birth months in common? Be sure to show your steps.
:::

::: {.callout-tip collapse="true"}
### Solution

It is easier to work with complements when you see the phrase "at least". So that we get $$\begin{align*}
P(\text{at least one shared}) 
&= 1 - P(\text{all different}) \\
&= 1 - \frac{12 \times 11 \times 10}{12^3} \\
&= 1 - \frac{11 \times 10}{12^2} \\
&= 1 - \frac{110}{144} \\
&= 1 - 0.7639 \\
&= \boxed{0.2361}
\end{align*}$$

------------------------------------------------------------------------

Alternatively, let \begin{align*}
A &= \text{first student is assigned any birth month (no restriction)} \\
B &= \text{second student has a different birth month from the first} \\
C &= \text{third student has a different birth month from both the first and second}
\end{align*}

Then, \begin{align*}
P(A \cap B \cap C) 
&= P(A) \cdot P(B \mid A) \cdot P(C \mid A \cap B) \\
&= 1 \cdot \frac{11}{12} \cdot \frac{10}{12} \\
&= \frac{110}{144} \\
&\approx 0.7639
\end{align*}

So the probability that at least two students share a birth month is: \begin{align*}
P(\text{at least one shared}) 
&= 1 - P(\text{all different}) \\
&= 1 - 0.7639 \\
&= \boxed{0.2361}
\end{align*}
:::

------------------------------------------------------------------------

# Week 2

::: {.callout-question collapse="true"}
### Question 1

Suppose that the moment generating function of $Y$ is given by $$M_Y(t) = e^{(a+b)(e^t - 1)}$$

where $a > 0$ and $b > 0$. Use this mgf to find the

a)  expected value of $Y$ and
b)  the variance of $Y$.
:::

::: {.callout-tip collapse="true"}
### Solution
a) The expected value of $Y$ is the first derivative of the mgf evaluated at $t = 0$:
$$\mathbb{E}[Y] = M_Y'(0)$$
Hence, 
$$M_Y'(t) = \frac{d}{dt} e^{(a+b)(e^t - 1)} = e^{(a+b)(e^t - 1)} \cdot (a+b)e^t$$
Evaluated at $t = 0$:
$$\mathbb{E}(Y) = M_Y'(0) = e^{(a+b)(1 - 1)} \cdot (a+b) \cdot 1 = (a + b)$$

------------------------------------------------------------------------

b) The variance of $Y$ is:
$$\mathrm{Var}(Y) = M_Y''(0) - \left( M_Y'(0) \right)^2$$

Hence, 
\begin{align*}
M_Y''(t) &= \frac{d}{dt} \left[ e^{(a+b)(e^t - 1)} \cdot (a+b)e^t \right] \\
&= e^{(a+b)(e^t - 1)} \cdot (a+b)e^t \cdot \left[ (a+b)e^t + 1 \right]
\end{align*}

Evaluate at $t = 0$:
$$M_Y''(0) = 1 \cdot (a+b) \cdot \left[ (a+b)(1) + 1 \right] = (a + b)^2 + (a + b)$$
Lastly,
$$\mathrm{Var}(Y) = (a + b)^2 + (a + b) - (a + b)^2 = a + b$$
:::

::: callout-note
**Fact: Uniqueness of mgfs**.
If two random variables $X$ and $Y$ have mgfs $M_X(t)$ and $M_Y(t)$, and there exists an open interval around $t = 0$ where both mgfs exist and are equal, then $$X = Y$$ That is, $X$ and $Y$ have the same distribution.
:::



::: {.callout-question collapse="true"}
### Question 2

Suppose $X$ has a Discrete Uniform Distribution over the values $x = 1,2,3,4,5$.

a) Sketch the pmf and cdf. 
b)  Find the mgf of $X$.
c)  Compute $\mathbb{E}(X)$ and $\mathbb{V}(X)$ two ways: by using the definitions, and by using the mgf you found in part b.
:::

::: {.callout-tip collapse="true"}
### Solution
To plot the pmf and cdf in R:
```{r}
x <- 1:5

# PMF: Each has equal probability
pmf <- rep(1/5, length(x))

# CDF: Cumulative sum of PMF
cdf <- cumsum(pmf)

# Plot PMF
plot(x, pmf, type="h", lwd=2, col="blue", ylim=c(0, 1),
     main="PMF of X", ylab="P(X = x)", xlab="x")
points(x, pmf, pch=16, col="blue")

# Plot CDF
plot(x, cdf, type="s", lwd=2, col="red",
     main="CDF of X", ylab="P(X ≤ x)", xlab="x")
points(x, cdf, pch=16, col="red")

```
Next, we find the mgf. 
By definition,

$$M_X(t) = \mathbb{E}[e^{tX}] = \sum_{x=1}^{5} e^{tx} \cdot \frac{1}{5} = \frac{1}{5} \left( e^t + e^{2t} + e^{3t} + e^{4t} + e^{5t} \right)$$

Lastly, we compute $\mathbb{E}(X)$ and $\mathbb{V}(X)$.

First way: by definition

$$\mathbb{E}(X) = \frac{1}{5}(1 + 2 + 3 + 4 + 5) = \frac{15}{5} = 3$$
$$\mathbb{E}(X^2) = \frac{1}{5}(1^2 + 2^2 + 3^2 + 4^2 + 5^2) = \frac{1}{5}(55) = 11$$
$$\mathrm{Var}(X) = \mathbb{E}(X^2) - (\mathbb{E}(X))^2 = 11 - 3^2 = 11 - 9 = 2$$

Second way: MFG approach
$$M_X(t) = \frac{1}{5} \left( e^t + e^{2t} + e^{3t} + e^{4t} + e^{5t} \right)$$
First derivative and evaluate at $t = 0$
$$M_X'(t) = \frac{1}{5} \left( e^t \cdot 1 + e^{2t} \cdot 2 + e^{3t} \cdot 3 + e^{4t} \cdot 4 + e^{5t} \cdot 5 \right)$$

$$M_X'(0) = \frac{1}{5}(1 + 2 + 3 + 4 + 5) = 3$$
Second derivative and evaluate at $t = 0$
$$M_X''(t) = \frac{1}{5} \left( e^t \cdot 1^2 + e^{2t} \cdot 2^2 + e^{3t} \cdot 3^2 + e^{4t} \cdot 4^2 + e^{5t} \cdot 5^2 \right)$$
$$M_X''(0) = \frac{1}{5}(1 + 4 + 9 + 16 + 25) = \frac{55}{5} = 11$$

Lastly,
$$\mathrm{Var}(X) = M_X''(0) - \left( M_X'(0) \right)^2 = 11 - 9 = 2$$

:::

::: {.callout-question collapse="true"}


### Question 3

Of the thousands of the volunteers who donate blood at a clinic, $80\%$ have the Rhesus (Rh) factor present in their blood. For the purposes of this question, assume the population size is very large (ie., we can use methods that assume sampling with replacement).

a)  If six volunteers are randomly selected, what is the probability that no one has the Rh factor?

b)  If six volunteers are randomly selected, what is the probability that at most four have the Rh factor?

c)  What if we were interested in the smallest number of volunteers who must be randomly selected if we want to be at least $90\%$ certain we will obtain at least four donors with the Rh factor? Can we answer this question using a Binomial Distribution? If so, answer this question. If not, explain why a Binomial distribution is not an appropriate model in this situation.
:::

::: {.callout-tip collapse="true"}
### Solution
We model this situation using the Binomial distribution, which is appropriate when the population is large, the number of trials $n$ is fixed, outcomes are binary (success or failure), and trials are independent. Although the true model is hypergeometric due to sampling without replacement, the large population size makes the Binomial a good approximation, as sampling with and without replacement become nearly equivalent (in fact, one can show via the pmfs that $\lim_{N \to \infty}$ Hypergeometric$(N, K = pN, n)$ = Binomial $(n, p)$).

With this in mind, let $X \sim$ binom$(n, p = 0.8)$ where $p =$ probability that a volunteer has the Rh factor. 

Let $X \sim \text{Binomial}(n = 6, p = 0.8)$, where $X$ is the number of people with the Rh factor. Then, $$P(X = 0) = \binom{6}{0}(0.8)^0(0.2)^6 = 1 \cdot 1 \cdot (0.2)^6 = \boxed{0.000064}$$

In R,
```{r}
dbinom(0, 6, 0.8)
```


Let $X \sim \text{Binomial}(n = 6, p = 0.8)$, where $X$ is the number of people with the Rh factor. Then $$P(X \leq 4) = 1 - P(X = 5) - P(X = 6) $$. Note that


\begin{aligned}
P(X = 5) &= \binom{6}{5}(0.8)^5(0.2)^1 = 6 \cdot 0.32768 \cdot 0.2 = 0.39322 \\
P(X = 6) &= \binom{6}{6}(0.8)^6(0.2)^0 = 1 \cdot 0.262144 \cdot 1 = 0.26214 \\
\end{aligned}


Hence, $$P(X \leq 4) = 1 - 0.39322 - 0.26214 = \boxed{0.34464}$$

In R,
```{r}
pbinom(4, 6, 0.8)
```


In this scenario, the number of trials $n$ is not fixed anymore since we are adjusting $n$ to find the smallest number of volunteers. Hence, a binomial distribution would not be appropriate anymore (see part a solution for a detailed requirement for modelling using binomial distribution). 

We would use trial and error approach, computing $P(X \ge 4)$ for different binomial distributions to determine the smallest $n$ where $P(X \ge 4)$ is at least $0.9$.

:::
